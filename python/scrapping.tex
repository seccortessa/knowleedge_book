\chapter{Scrapping}

    \section{Introducción al Scrapping}

    El web scraping es una técnica poderosa utilizada para extraer información de páginas web. Esta técnica emplea programas o scripts para simular la navegación de un usuario en la web, accediendo a páginas web y extrayendo los datos necesarios de ellas. La flexibilidad del web scraping permite su aplicación en una variedad de contextos, desde la recopilación de datos para análisis de mercado hasta la automatización de tareas de recolección de datos para investigación académica o desarrollo de productos.

    Una característica notable del web scraping es su capacidad para interactuar tanto con páginas web estáticas como dinámicas. Las páginas estáticas son aquellas cuyo contenido no cambia con cada solicitud y generalmente se sirven directamente desde el servidor en formato HTML. Por 9otro lado, las páginas dinámicas son aquellas cuyos contenidos pueden cambiar en función de la interacción del usuario, los datos de entrada, o incluso el tiempo, y a menudo implican la carga de datos a través de APIs o tecnologías de frontend como JavaScript.

    Además, el web scraping moderno puede manejar desafíos más complejos como el análisis y la interacción con APIs, así como la superación de CAPTCHAs, que son mecanismos diseñados para diferenciar entre usuarios humanos y bots automatizados. El manejo de CAPTCHAs es un área avanzada del web scraping, que a menudo implica técnicas sofisticadas y, en algunos casos, cuestiones éticas y legales.

    En el corazón del web scraping se encuentran varias herramientas y tecnologías clave. Bibliotecas como BeautifulSoup (bs4) y Scrapy en Python son ampliamente utilizadas para parsear y navegar por el contenido de las páginas web. BeautifulSoup es particularmente conocida por su simplicidad y eficiencia en extraer datos de HTML y XML, mientras que Scrapy ofrece un marco más completo para la creación de spiders - programas que automatizan la navegación y extracción de datos de múltiples páginas.

    La biblioteca \texttt{requests} en Python es otra herramienta esencial, que se utiliza para realizar solicitudes HTTP a servidores web. Esta biblioteca simplifica el proceso de envío de solicitudes y manejo de respuestas, lo cual es fundamental para acceder a los contenidos de las páginas web.

    Un aspecto crucial del web scraping es el análisis y la selección de los datos específicos a extraer. Aquí es donde entra en juego XPath, un lenguaje que permite navegar a través de los elementos y atributos en los documentos HTML y XML. XPath proporciona una forma poderosa y flexible de identificar y extraer partes específicas de una página web, lo que facilita enormemente la tarea de localizar y recopilar los datos deseados.

    En resumen, el web scraping es una técnica multifacética que se ha vuelto indispensable en el mundo del análisis de datos y la automatización. Con la ayuda de herramientas como BeautifulSoup, Scrapy, `requests`, y el uso de XPath para el análisis de HTML, los desarrolladores y analistas pueden extraer una gran cantidad de información de la web, abarcando desde páginas web estáticas sencillas hasta páginas dinámicas y APIs complejas. Sin embargo, es crucial tener en cuenta las implicaciones legales y éticas al realizar web scraping, asegurándose de respetar las políticas de uso de los sitios web y las leyes de protección de datos.
    
        \subsection{XPath}

        XPath, que significa XML Path Language, es un lenguaje de consulta que se utiliza para seleccionar nodos de un documento XML. Este lenguaje ofrece una forma precisa y flexible de navegar y localizar partes específicas de un documento, lo cual es esencial en el contexto del procesamiento de XML y HTML, especialmente en tareas como el web scraping y la transformación de datos. XPath utiliza una sintaxis de ruta similar a las rutas de archivo en sistemas operativos, lo que permite a los usuarios y desarrolladores especificar patrones para identificar nodos, atributos y valores dentro del documento XML.

        La fuerza de XPath radica en su capacidad para realizar búsquedas tanto específicas como generales dentro de un documento. Por ejemplo, puede seleccionar todos los nodos que cumplen con cierto criterio, o puede enfocarse en un solo elemento con una identificación única. Además, XPath soporta funciones integradas para cadenas, números y lógica booleana, lo que añade una capa adicional de potencia y versatilidad a las consultas. Esta combinación de flexibilidad y precisión hace de XPath una herramienta indispensable en muchas aplicaciones que involucran XML, como la transformación de datos con XSLT, la configuración de ciertos frameworks de desarrollo de software y, más comúnmente, la extracción de datos específicos de páginas web en el web scraping. Con su enfoque en la estructura del documento y su capacidad para manejar consultas complejas, XPath se ha establecido como un estándar para la manipulación y consulta de documentos XML.

        La siguiente es una lista de los comandos más utilizados en XPath:

        \begin{itemize}
            \item \texttt{/}: Selector de raíz, selecciona desde el nodo raíz.
            \item \texttt{//}: Selecciona nodos en todo el documento desde el nodo actual que coinciden con la selección.
            \item \texttt{.}: Selecciona el nodo actual.
            \item \texttt{..}: Selecciona el nodo padre del nodo actual.
            \item \texttt{@}: Selecciona atributos. Ejemplo: \texttt{@class}.
            \item \texttt{*}: Selecciona todos los nodos hijos del nodo actual.
            \item \texttt{node0()}: Selecciona todos los tipos de nodos bajo el nodo actual.
            \item \texttt{[]}: Aplica un filtro, seleccionando elementos que cumplen el criterio dentro de los corchetes.
            \item \texttt{|}: Combina expresiones y devuelve la unión de sus resultados.
            \item \texttt{text()}: Selecciona todos los nodos de texto bajo el nodo actual.
            \item \texttt{@*[local-name()='nombre']}: Selecciona atributos con un nombre local específico.
            \item \texttt{[n]}: Selecciona el enésimo nodo en un conjunto.
            \item \texttt{[last()]}: Selecciona el último nodo en un conjunto.
            \item \texttt{[position()<n]}: Selecciona nodos en posiciones menores que \texttt{n}.
            \item \texttt{[contains(@atributo, 'texto')]}: Selecciona nodos con un atributo que contiene un texto dado.
            \item \texttt{[starts-with(@atributo, 'texto')]}: Selecciona nodos con un atributo que comienza con un texto dado.
            \item \texttt{[string-length(@atributo)>n]}: Selecciona nodos con la longitud de cadena de un atributo mayor que \texttt{n}.
            \item \texttt{[not(expresión)]}: Selecciona nodos que no cumplen con la expresión dada.
        \end{itemize}

        
    \section{Scrapping para una sola página estática}

    La extracción de los datos se realiza en dos fases; primero se realiza una solicitud o requerimiento para obtener la respuesta que corresponde al HTML de la página. Una vez se obtiene esta, es necesario analizarlo. El análisis se realiza mediante la librería \texttt{lxml}.

    \begin{verbatim}
        import requests
        from lxml import html

        headers = {
        "user-agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36"
        }

        url = "http://www.wikipedia.org"

        response = requests.get(url, headers = headers)
        parser = html.fromstring(response.text)
    \end{verbatim}

    El \texttt{parser} es un objeto del tipo \texttt{html} del cual se puede extraer la información que sea requerida. Por ejemplo, de la página que estamos analizando podemos ver que hay una serie de idiomas, queremos extraer esta información. Inspeccionando la página podemos ver que todos los idiomas están dentro de una etiqueta \texttt{<strong>} dentro de una etiqueta \texttt{a} cuya id es \texttt{"js-link-box-es"}, así que intentamos extraer esta informacón:

    \begin{verbatim}
        spanish = parser.get_element_by_id("js-link-box-es")
        print(spanish.text_content())
    \end{verbatim}

    También es posible recuperar la información con comandos de XPath:

    \begin{verbatim}
        spanish = parser.xpath("//a[@id = 'js-link-box-es']/strong/text()")
        print(spanish)
    \end{verbatim}

    Ahora, en este caso particular se puede extraer toda la lista xon los idiomas usando xpath, entendiendo que todos los idiomas están dentro de un div con \texttt{class = 'central-featured-lang'}:

    \begin{verbatim}
        languages = parser.xpath("//div[contains(@class, 'central-featured-lang')]//strong/text()")
        print(languages)
    \end{verbatim}
 
    Por su parte, la librería \texttt{lxml} también provee un extractor para las clases y con esto se puede obtener la misma lista:

    \begin{verbatim}
        idiomas = parser.find_class("central-featured-lang")
        for idioma in idiomas:
            print(idioma.text_content())
    \end{verbatim}
    
    Para el buscador de \texttt{xpath}, las clases definidas para los divs con los textos de losd idiomas son \texttt{"central-featured-lang lang1"}, etc. Por eso para extraerlos todos se ha utilizado la palara \texttt{contains}, sin embargo, \texttt{lxml} conoce que el espacio para cada div, indica una clase adicional, al solamente rquerir la clase común, se retorna la lista con todos los divs. \\

    La herramienta \texttt{beautiful soup} es muy similar a la de \texttt{lxml} par aextraer datos, un ejemplo de uso sería al siguiente:

    \begin{verbatim}
        
    \end{verbatim}

    El método \texttt{find()} de Beautiful Soup es una función esencial en la biblioteca para analizar y manipular HTML y XML. Se utiliza para buscar el primer elemento que coincide con un criterio de búsqueda específico en un documento HTML o XML.

    \begin{itemize}
        \item \textbf{Función Básica:} 
            \textit{find()} busca en el árbol de elementos del documento y devuelve el primer elemento que coincide con los criterios especificados.
          
        \item \textbf{Sintaxis Básica:} 
            \textit{find(name, attrs, recursive, string, **kwargs)}
            \begin{itemize}
                \item \textit{name}: El nombre de la etiqueta que quieres buscar (por ejemplo, 'div', 'span').
                \item \textit{attrs}: Un diccionario de atributos y valores para buscar en las etiquetas (por ejemplo, \{'class': 'some-class'\}).
                \item \textit{recursive}: Si es \textit{True} (por defecto), busca en todos los hijos del elemento; si es \textit{False}, sólo busca en los hijos directos.
                \item \textit{string}: Se utiliza para buscar una cadena de texto dentro de los elementos.
                \item \textit{**kwargs}: Argumentos adicionales basados en atributos de etiquetas específicas (por ejemplo, id='element-id').
            \end{itemize}
      
        \item \textbf{Uso Típico:}
            \begin{itemize}
                \item Para buscar la primera etiqueta \textit{<div>} con una clase específica, se usaría: \texttt{soup.find('div', class\_='nombre-clase')}
                \item Si se busca un elemento con un \textit{id} específico: \textit{soup.find(id='element-id')}
            \end{itemize}
      
        \item \textbf{Retorno:} 
            Devuelve el primer elemento que coincide con los criterios o \textit{None} si no se encuentra ningún elemento.
      
        \item \textbf{Uso Combinado con Otros Métodos:} 
            A menudo se usa en combinación con métodos como \textit{find\_all()} o métodos de navegación de árboles para acceder a elementos específicos de la estructura del documento.
    \end{itemize}
      
    \texttt{Beautiful Soup} tiene una verntaja sobre \texttt{lxml}, y es que en caso de que alguna etiqueta no contenga un identificador o alguna clase que sea sencillo de obtener, pero la etiqueta prima anterior sí contiene una, se puede usar el método \texttt{find\_next\_sibling('div')}. \\\




    Scrapy es un framework mucho más completo de extracción de datos. Para extraer datos de html de una página, se debe definir una clase heredada de \texttt{Item} en la cual se definan los campos necesarios o requeridos (esto es en función del tipo de información). En el ejemplo, se desea extraer las preguntas de la pagina, y esta tiene la pregunta principal y un texto pequeño de descripción, entonces se definen esos campos:

    \begin{verbatim}
        from scrapy.item import Field
        from scrapy.item import Item
        from scrapy.spiders import Spider
        from scrapy.selector import Selector
        from scrapy.loader import ItemLoader
            
        class Question(Item):
            quest_title = Field()
            quest_description = Field()
            
        class StackSpider(Spider):
            name = "SpiderForStackOverflow"
            custom_settings = {
                'USER_AGENT':   'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 
                                (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'
            }
            start_urls = ["http://stackoverflow.com/questions/"]
            
            def parse(self, response):
                sel = Selector(response)
                questions =     sel.xpath("//div[@id = 'questions']//div[@class = 
                                's-post-summary--content']")
                print(questions)
                for question in questions:
                    item = ItemLoader(Question(), question)
                    item.add_xpath("quest_title", "./h3/a/text()")
                    item.add_xpath("quest_description", ".//div[@class = 's-post-
                                    summary--content-excerpt']/text()")
                    yield item.load_item()
        #   Para ejecutar el script debemos poner en consola C:\Users\seb-c\AppData\
            local\packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache
            \local-packages\Python310\Scripts\scrapy.exe runspider scrapy_prb.py -o 
            questions.json -t json
    \end{verbatim}

    En la clase \texttt{StackSpider} se definen los parámetros de la extracción. Notar cómo se implementa el xpath para recuperar la onformación necesaria y cómo se almacenan en los campos definidos para la clase personalizada \texttt{Question}. \\

    Por último, es posible configurar ael ejecutador del crawler para que se realice el sctapping sin necesidad de ejecutar comandos en la consola:

    \begin{verbatim}
        from scrapy.crawler     import CrawlerProcess
                       
        process = CrawlerProcess(
            {
                'FEED_Format': 'json',
                'FEED_URI': 'results.json'
            }
        )
        process.crawl(StackSpider)
    \end{verbatim}

    \section{Extracción vertical y horizontal de páginas}

        En muchas páginas podemos ver secciones en las que se enlista un conjunto de items y, usualmente resultados de bpuaquedsa y estos se organizan de modo que quedan en varias páginas. Esto es denominado como paginación y se puede lograr la extracción de estas páginas. Esta es la denominada extracción horizontal.

        El proceso de la extracción suele empezar con relizar un 'scrolling' vertical y a partir de aho ir cambiando de páginas. El primer ejemplo será realizara este srolling vertical en una página de tripadvisor.

        Paso a paso, realizamos la extracción mediante \texttt{Scrapy}. Primero se importan los módulos requeridos

        \begin{verbatim}
        from scrapy.item import Field
        from scrapy.item import Item
        from scrapy.spiders import CrawlSpider, Rule
        from scrapy.selector import Selector
        from scrapy.loader.processors import MapCompose
        from scrapy.linkextractors import LinkExtractors
        from scrapy.loader import ItemLoader        
        \end{verbatim}

        Se define una clase heredada de \texttt{Item}

        \begin{verbatim}
        class Restaurant(Item):
            name        = Field()
            price       = Field()
            description = Field()
            amenities   = Field()
        \end{verbatim}

        Ahora se defineotra clase heredada de \texttt{CrawlSpider}

        \begin{verbatim}
        class TripAdvisor(CrawlSpider):
            name = "Restaurants"
            custom_settings = {'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) 
            AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}
        \end{verbatim}